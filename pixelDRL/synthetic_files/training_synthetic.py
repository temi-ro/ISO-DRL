# File entirely generated by Google Gemini 3 (For more info, see report Section 7.1)

# This script trains the PixelDRL model on a synthetic dataset of simple shapes (squares and circles).
# The dataset generates images of size 240x240 with random shapes and noise.
# The training loop includes a debug function to calculate the Dice score on a small validation set after each epoch, allowing us to monitor the model's performance on a simple segmentation task. 
# The model learns to predict a binary mask that matches the shape in the image, and the reward is based on the improvement of the mask compared to the ground truth. 
# Checkpoints are saved after each epoch for later analysis.

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
import argparse
import os  
from model import PixelDRL 
from dataset_synthetic import SyntheticShapesDataset
from torch.utils.data import DataLoader
import datetime

# --- HELPER: Dice Score Calculation ---
def calculate_dice(pred, target):
    """
    Computes the Dice Coefficient for a single batch.
    Robustly handles 3D [B, H, W] and 4D [B, C, H, W] inputs.
    """
    # 1. Ensure both inputs are 4D [Batch, Channel, Height, Width]
    if pred.dim() == 3:
        pred = pred.unsqueeze(1)
    if target.dim() == 3:
        target = target.unsqueeze(1)

    smooth = 1e-5
    
    # 2. Now it is safe to sum over dimensions (1, 2, 3)
    intersection = (pred * target).sum(dim=(1, 2, 3))
    union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3))
    
    dice = (2. * intersection + smooth) / (union + smooth)
    return dice.mean().item()

def train_one_episode(model, optimizer, image, ground_truth, t_max=10, gamma=0.95):
    # Ensure model is in training mode
    model.train()
    batch_size = image.size(0)
    device = image.device
    
    optimizer.zero_grad()

    # 1. INITIALIZE MASK (All Ones -> Pruning Strategy)
    current_mask = torch.ones_like(image).to(device)
    
    log_probs = []
    values = []
    rewards = []
    entropies = []
    
    # --- EPISODE LOOP ---
    for t in range(t_max):
        state = torch.cat([image, current_mask], dim=1)
        
        # Forward pass
        policy_logits, value_map = model(state)
        
        probs = torch.sigmoid(policy_logits)
        dist = torch.distributions.Bernoulli(probs=probs)
        action = dist.sample() # image of shape [B, 1, H, W]
        
        # DESTRUCTIVE ACTION: Only keep pixels where action=1
        new_mask = current_mask * action
        
        # Reward Calculation
        prev_diff = torch.square(current_mask - ground_truth)
        curr_diff = torch.square(new_mask - ground_truth)
        
        reward = (prev_diff - curr_diff) * 10.0
        # Log Scaling Reward
        # reward_scalar = torch.sign(reward_scalar) * torch.log(1 + torch.abs(reward_scalar))

        log_probs.append(dist.log_prob(action))
        values.append(value_map.squeeze(1)) # [B, H, W]
        entropies.append(dist.entropy())
        
        # Expand reward to match image dimensions
        rewards.append(reward.squeeze(1))  # [B, H, W]
        
        current_mask = new_mask.detach()

    # --- LOSS CALCULATION ---
    R = torch.zeros_like(values[0])
    policy_loss = 0
    value_loss = 0
    
    for i in reversed(range(t_max)):
        R = rewards[i] + gamma * R
        advantage = R - values[i]
        
        policy_loss -= (log_probs[i] * advantage.detach()).mean()
        policy_loss -= 0.01 * entropies[i].mean() # Reduced entropy weight

        value_loss += torch.square(advantage).mean()

    total_loss = policy_loss + 0.5 * value_loss
    
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    
    return total_loss.item(), torch.stack(rewards).mean().item()

# --- NEW FUNCTION: Debug Dice Check ---
def debug_dice_check(model, dataloader, device, t_max=10):
    model.eval()
    try:
        # Get just one batch of data
        images, ground_truths, _ = next(iter(dataloader))
        images = images.to(device)
        ground_truths = ground_truths.to(device)
        
        # Start with all ones
        current_mask = torch.ones_like(images).to(device)
        
        # Run inference loop (No Grad)
        with torch.no_grad():
            for t in range(t_max):
                state = torch.cat([images, current_mask], dim=1)
                policy_logits, _ = model(state)
                probs = torch.sigmoid(policy_logits)
                
                # Deterministic Action for Evaluation (Threshold 0.5)
                action = (probs > 0.5).float()
                current_mask = current_mask * action
        
        # Calculate Final Dice
        dice_score = calculate_dice(current_mask, ground_truths)
        print(f"\n[DEBUG] Validation Dice Score (Batch Size {images.size(0)}): {dice_score:.4f}")
        print(f"[DEBUG] Ground Truth Pixels: {ground_truths.sum().item():.0f} | Pred Pixels: {current_mask.sum().item():.0f}\n")
        
    except Exception as e:
        print(f"[DEBUG] Could not run Dice Check: {e}")
    
    model.train() # Switch back to train mode

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train PixelDRL model on BraTS2023 dataset")
    parser.add_argument("--dataset", type=str, required=True, help="Path to the dataset file")
    parser.add_argument("--resume", type=str, default=None, help="Path to checkpoint .pth file to resume from")
    
    args = parser.parse_args()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_gpus = torch.cuda.device_count()
    print(f"Using device: {device}")

    # Model Init (n_actions=1 for Bernoulli)
    model = PixelDRL(in_channels=2, n_actions=1).to(device)
    
    if num_gpus > 1:
        model = nn.DataParallel(model)

    # Resume Logic
    start_epoch = 0
    if args.resume is not None:
        if os.path.isfile(args.resume):
            print(f"Loading checkpoint '{args.resume}'...")
            checkpoint = torch.load(args.resume, map_location=device)
            if isinstance(model, nn.DataParallel):
                model.module.load_state_dict(checkpoint, strict=False)
            else:
                model.load_state_dict(checkpoint, strict=False)
            try:
                fname = os.path.basename(args.resume)
                if "epoch" in fname:
                    start_epoch = int(fname.split("epoch")[1].split(".pth")[0]) + 1
            except:
                pass
        else:
            print(f"Error: Checkpoint file '{args.resume}' not found!")
            exit()

    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.9)

    if start_epoch > 0:
        for _ in range(start_epoch):
            scheduler.step()

    per_gpu_batch_size = 4
    total_batch_size = per_gpu_batch_size * max(1, num_gpus)
    
    train_dataset = SyntheticShapesDataset(size=1000, img_size=240)
    
    # We create a tiny separate validation loader for the debug check
    val_dataset = SyntheticShapesDataset(size=200, img_size=240) # Using smaller subset for quick debug
    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True) # Small batch for debug

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=total_batch_size, 
        shuffle=True, 
        num_workers=3 * num_gpus if device.type == "cuda" else 0, 
        pin_memory=True if device.type == "cuda" else False
    )
    
    print(f"Starting Training from Epoch {start_epoch}...")
    num_epochs = 100
    
    for epoch in range(start_epoch, num_epochs):
        loss = -1.
        for batch_idx, (images, rl_targets, binary_masks) in enumerate(train_dataloader):
            images = images.to(device)
            rl_targets = rl_targets.to(device)
            
            loss, reward_mean = train_one_episode(model, optimizer, images, rl_targets, t_max=10, gamma=0.95)

            if batch_idx % 2500 == 0:
                print(f"[{datetime.datetime.now()}]: Epoch [{epoch}/{num_epochs}] Batch {batch_idx} Loss: {loss:.4f}, Reward Mean: {reward_mean:.4f}")        

        scheduler.step()
        print(f"[{datetime.datetime.now()}][*]: Epoch [{epoch}/{num_epochs}], Loss: {loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")        
        
        # --- NEW: RUN DEBUG DICE CHECK ---
        debug_dice_check(model, val_dataloader, device)
        # ---------------------------------

        save_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
        save_path = f"/vol/biomedic2/bglocker_studproj/trm25/checkpoints/pixelDRL_synthetic_epoch{epoch}.pth"
        torch.save(save_state, save_path)

    final_state = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()
    torch.save(final_state, "/vol/biomedic2/bglocker_studproj/trm25/checkpoints/pixelDRL_synthetic_final.pth")